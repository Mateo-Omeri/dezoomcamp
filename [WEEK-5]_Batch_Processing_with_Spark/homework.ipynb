{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Print spark version\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write csv data into GCS with partitions\n",
    "\n",
    "from pyspark.sql import types \n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('gs://your-bucket-name/homework/fhv_tripdata_2019-10.csv')\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.repartition(6)\n",
    "\n",
    "df.write.parquet('gs://your-bucket-name/homework/fhvhv/2019/10/')\n",
    "\n",
    "print(\"Job finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count taxi trips were there on the 15th of October\n",
    "\n",
    "df = spark.read.parquet('gs://your-bucket-name/homework/fhvhv/2019/10/')\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.registerTempTable('trips_data')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(*)\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE \n",
    "    DATE(pickup_datetime) = '2019-10-15'\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"Job finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Longest trip for each day\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT MAX(TIMESTAMPDIFF(HOUR, pickup_datetime, dropoff_datetime)) AS longest_trip_hours\n",
    "FROM trips_data\"\"\").show()\n",
    "\n",
    "print(\"Job finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count taxi trips were there on the 15th of October\n",
    "\n",
    "df_zone = spark.read.option(\"header\", \"true\").csv('gs://your-bucket-name/homework/taxi_zone_lookup.csv')\n",
    "\n",
    "df_zone.show()\n",
    "\n",
    "df_zone.registerTempTable('taxi_zone_lookup')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    zl.Zone AS least_frequent_pickup_location\n",
    "FROM\n",
    "    taxi_zone_lookup zl\n",
    "JOIN\n",
    "    trips_data td\n",
    "ON\n",
    "    zl.LocationID = td.PULocationID\n",
    "GROUP BY\n",
    "    zl.Zone\n",
    "ORDER BY\n",
    "    COUNT(td.PULocationID)\n",
    "LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"Job finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
